{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "n6g1CJDpQ3p-",
        "aL2qNGBCOJvj",
        "WoEzSrzfOU_L",
        "Xh5oK1gFOcux",
        "QTGAj-0toAjG",
        "eyMrbAY5Knzn",
        "afq9C1gZKu1a",
        "Oa3yaUoM_d2F"
      ],
      "authorship_tag": "ABX9TyOX6nMSYz4lz/HHwVe/lwAV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DSShail/Machine-Learning/blob/main/credit_card_fraud_detection_2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "n6g1CJDpQ3p-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJxcGO_c_rf0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "\n",
        "from statsmodels.graphics.gofplots import qqplot\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score,accuracy_score,classification_report,accuracy_score,confusion_matrix\n",
        "!pip install statsmodels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Acquisition"
      ],
      "metadata": {
        "id": "aL2qNGBCOJvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Reading the CSV file into a dataframe*"
      ],
      "metadata": {
        "id": "Vhvh0UMPcHhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "credit_df=pd.read_csv('creditcard.csv')"
      ],
      "metadata": {
        "id": "_rPYg9uRA61G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Dropping the Na rows and columns *"
      ],
      "metadata": {
        "id": "bVNiir_UcN-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Preprocessing\n",
        "\n",
        "**we will check if the data needs transformation**\n",
        "*  label encoding\n",
        "*  ordinal encoding\n",
        "*  column transformation\n",
        "*  function transformer\n",
        "*  power transformer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WoEzSrzfOU_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "credit_df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "y2N_OZE4FGB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data Visualization"
      ],
      "metadata": {
        "id": "Xh5oK1gFOcux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting heatmap to check the relationships in the dataset**"
      ],
      "metadata": {
        "id": "kG9pEdTJOiUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corelation_matrix=credit_df.corr()\n",
        "plt.figure(figsize=(14,8))\n",
        "sns.heatmap(corelation_matrix,annot=False,cmap='coolwarm',linewidths=1.5)\n",
        "plt.title('Corelation heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UwXlfKRKOh6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*training the data into train and test data*"
      ],
      "metadata": {
        "id": "8jCZKCRvbDs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X=credit_df.drop(columns=['Class'])\n",
        "y=credit_df['Class']\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\n"
      ],
      "metadata": {
        "id": "sLxfhp3eA5WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Looking at the data structure(column wise)*\n",
        "\n",
        "*we will use QQ plot for looking at the data distribution*\n"
      ],
      "metadata": {
        "id": "QQb2uPpjPuuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in X_train.columns:\n",
        "    fig,ax=plt.subplots(nrows=1,ncols=2,figsize=(14,6))\n",
        "    sns.distplot(X_train[col],ax=ax[0])\n",
        "    ax[0].set_title(f'KDE Plot for {col}')\n",
        "\n",
        "    qqplot(X_train[col],line='s',ax=ax[1])\n",
        "    ax[1].set_title(f'QQ Plot for {col}')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "pzG2SZnZP7SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Softmax Regression-Logistic Regression"
      ],
      "metadata": {
        "id": "QTGAj-0toAjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*We will apply softmax regression although it is used with multi-class classification problem statement*"
      ],
      "metadata": {
        "id": "MzUhP1bir5GZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we need to scale the input features before fitting into the model"
      ],
      "metadata": {
        "id": "hjc2A3WMtbnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scalar=StandardScaler()\n",
        "\n",
        "X_train_scaled=scalar.fit_transform(X_train)\n",
        "X_test_scaled=scalar.transform(X_test)"
      ],
      "metadata": {
        "id": "l4eVKPxRtj3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The hyper-parameter used in the softmax regression is*\n",
        "1.   max_iter\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "etld-8iSIN1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#using multi-class as multinomial the LogisticRegression becomes SOFTMAX REGRESSION\n",
        "LoR=LogisticRegression(multi_class='multinomial',max_iter=1000)\n",
        "LoR.fit(X_train_scaled,y_train)\n",
        "y_pred_lor=LoR.predict(X_test_scaled)\n",
        "print(accuracy_score(y_pred_lor,y_test))\n",
        "print(classification_report(y_pred_lor,y_test))"
      ],
      "metadata": {
        "id": "6onrHKhBoRU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   The recall is 77\n",
        "*   The precision for fraud is 69 precent\n",
        "\n"
      ],
      "metadata": {
        "id": "XMj3ktW2ueFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree"
      ],
      "metadata": {
        "id": "eyMrbAY5Knzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parmas={\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [5, 10, 15, 20, 25],\n",
        "    'min_samples_leaf': [1, 5],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "}"
      ],
      "metadata": {
        "id": "dO2vuPztAll0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Appying GridSearchCV on DecisionTreeClassifier**"
      ],
      "metadata": {
        "id": "NAE_6crhbK_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt_clf=DecisionTreeClassifier()\n",
        "\n",
        "grid_search=GridSearchCV(estimator=dt_clf, param_grid=parmas, cv=5)\n",
        "grid_search.fit(X_train,y_train)\n",
        "\n",
        "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
        "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
      ],
      "metadata": {
        "id": "15ERz4XxBOBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*we will use the output of GridSearchCV and Hypertune the decision tree parameters*"
      ],
      "metadata": {
        "id": "IyLWMO2abfhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Hyperparamters used in the decision tree are\n",
        "  1.   criterion\n",
        "  2.   max_depth\n",
        "  3.   min_samples_leaf\n",
        "  4.   min_samples_split"
      ],
      "metadata": {
        "id": "T41Dz7RlIGEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "#decison tree with hyperparameter tuining\n",
        "decision_tree_clf=DecisionTreeClassifier(criterion='gini')\n",
        "decision_tree_clf =decision_tree_clf.fit(X_train,y_train)\n",
        "y_pred_dt=decision_tree_clf.predict(X_test)"
      ],
      "metadata": {
        "id": "d8VoKP60Bbg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Calculating the accuracy and classification report*"
      ],
      "metadata": {
        "id": "KysVyFmsbqLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "print(classification_report(y_test,y_pred_dt))\n",
        "\n",
        "print(accuracy_score(y_pred_dt,y_test))"
      ],
      "metadata": {
        "id": "WVlqp-kkDX9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   F1 score is 1- that means model is ideal\n",
        "*   The accuracy is also 100 percent\n",
        "\n"
      ],
      "metadata": {
        "id": "W2rfq9XIF7Ls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The biggest concern is the data - the dataset is imbalanced so we need to work on imbalanced dataset*"
      ],
      "metadata": {
        "id": "H0ydmELL_Rz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest - Bagging Ensemble"
      ],
      "metadata": {
        "id": "afq9C1gZKu1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Appying GridSearchCV on RandomForest*"
      ],
      "metadata": {
        "id": "iQfanG4Tb1C7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_parmas={\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'n_estimators':[5,10,15,20],\n",
        "    'max_depth': [5, 10, 15, 20, 25],\n",
        "    'min_samples_leaf': [1, 5],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "}"
      ],
      "metadata": {
        "id": "K2gHOK3hGjdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf1=RandomForestClassifier()\n",
        "grid_search=GridSearchCV(estimator=rf1, param_grid=rf_parmas, cv=5)\n",
        "grid_search.fit(X_train,y_train)\n",
        "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
        "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
      ],
      "metadata": {
        "id": "oZSNXTBYF1fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Hyperparameters used for the random forest is\n",
        "  1.  n_estimators\n",
        "\n",
        "*   Hyperparamters used in the decision tree of the random forest are\n",
        "  1.   criterion\n",
        "  2.   max_depth\n",
        "  3.   min_samples_leaf\n",
        "  4.   min_samples_split\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IH9f9bKTGGnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rf2=RandomForestClassifier(n_estimators=5,criterion='gini',max_depth=5,min_samples_leaf=1,min_samples_split=2)\n",
        "rf2.fit(X_train,y_train)\n",
        "y_pred=rf2.predict(X_test)\n",
        "\n",
        "print(accuracy_score(y_pred,y_test))\n",
        "print(classification_report(y_pred,y_test))"
      ],
      "metadata": {
        "id": "J8I8m8Q0L2DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iiriBoURd3gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Importance using random Forest"
      ],
      "metadata": {
        "id": "Oa3yaUoM_d2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imblearn"
      ],
      "metadata": {
        "id": "uw02V6XiDsYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "credit_df=pd.read_csv('creditcard.csv')\n",
        "print(credit_df.shape)"
      ],
      "metadata": {
        "id": "TxjVnmNxLt_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(credit_df.isna().sum())\n",
        "credit_df.dropna(inplace=True)\n",
        "print(credit_df.isna().sum())"
      ],
      "metadata": {
        "id": "EcfBQ7JgmEsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Separate features and target variable\n",
        "X = credit_df.drop('Class', axis=1)\n",
        "y = credit_df['Class']\n",
        "\n",
        "# Apply SMOTE for oversampling\n",
        "#X.dropna(inplace=True)\n",
        "#y.dropna(inplace=True)\n",
        "\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,test_size=0.2)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "smote = SMOTE(random_state=12)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "#print(\"X_resampled: \", X_resampled)\n",
        "#print(\"y_resampled: \", y_resampled)"
      ],
      "metadata": {
        "id": "OyqX9A5LmgXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#For Feature importance\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Fit the model to determine feature importances\n",
        "rf.fit(X_resampled, y_resampled)\n",
        "\n",
        "\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf.feature_importances_\n",
        "\n",
        "# Display feature importances in a sorted manner\n",
        "feature_importances_sorted = sorted(zip(feature_importances, X_resampled.columns), reverse=True)\n",
        "\n",
        "#print(feature_importances_sorted)\n",
        "for importance, feature in feature_importances_sorted:\n",
        "    print(f\"Feature: {feature}, Importance: {importance}\")### Feature Importance using Random Forest"
      ],
      "metadata": {
        "id": "GEWlXTjq_pqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*We will select three most relevant features after appying feature importance. Those features are as given below*\n",
        "1.   V14\n",
        "2.   V10\n",
        "3.   V12\n",
        "\n",
        "We will create a dataframe(X_selected and y_selected) consisting of these above columns and perform the ML algo.\n",
        "\n"
      ],
      "metadata": {
        "id": "KBHqKrkyJ7jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_selected=X_resampled[['V14','V12','V10']]\n",
        "y_selected=y_resampled"
      ],
      "metadata": {
        "id": "HYzYn8LJK2-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*splitting the data into training set and test set*"
      ],
      "metadata": {
        "id": "eZcuYvusMESs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(X_selected,y_selected,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "T8TROYn0MDyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#created a dataframe based on X_selected and y_selected for the purpose of using it in the plotly\n",
        "X_selected_new=pd.concat([X_selected,y_selected],axis=1)\n",
        "X_selected_new.sample(10)"
      ],
      "metadata": {
        "id": "7BdzfRN13TyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "fig=px.scatter_3d(X_selected_new,x='V14',y='V12',z='V10',color='Class')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "DkBuEjzh5P8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression after feature importance"
      ],
      "metadata": {
        "id": "rpyR6woBL30b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scalar=StandardScaler()\n",
        "\n",
        "X_train_scaled=scalar.fit_transform(X_train)\n",
        "X_test_scaled=scalar.transform(X_test)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#using multi-class as multinomial the LogisticRegression becomes SOFTMAX REGRESSION\n",
        "LoR=LogisticRegression(multi_class='multinomial',max_iter=1000)\n",
        "LoR.fit(X_train_scaled,y_train)\n",
        "y_pred_lor=LoR.predict(X_test_scaled)\n",
        "print(accuracy_score(y_pred_lor,y_test))\n",
        "print(classification_report(y_pred_lor,y_test))\n",
        "print(confusion_matrix(y_pred_lor,y_test))"
      ],
      "metadata": {
        "id": "GDLwOO8VLJuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The accuracy is 1 for Logistic Regression(Softmax)\n",
        "* The precison and recall for both fraud and non-fraud is one that means that the model correctly classifies all the positive and neagtive instances\n",
        "\n"
      ],
      "metadata": {
        "id": "VcZ9Cd_31UG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting RoC Curve**\n",
        "\n",
        "*RoC curve is the graph of FPR Vs TPR*\n",
        "\n",
        "*   FPR is False Positive Rate\n",
        "*   TPR is True positive Rate"
      ],
      "metadata": {
        "id": "z6qrp9P0nPla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_lor)\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "display=metrics.RocCurveDisplay(fpr=fpr,tpr=tpr,roc_auc='roc_auc',\n",
        "                                estimator_name='example estimator')\n",
        "display.plot()\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "PYoT8A8PmIPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K Nearest Neighbours"
      ],
      "metadata": {
        "id": "iPeIQ-TJjQBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Although KNN is not the favourable for large dataset- high latency *"
      ],
      "metadata": {
        "id": "mg9XeQQkKJwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scalar=StandardScaler()\n",
        "\n",
        "X_train=scalar.fit_transform(X_train)\n",
        "X_test=scalar.transform(X_test)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "#no is neighbours is 5 - default config.\n",
        "knn=KNeighborsClassifier()\n",
        "\n",
        "knn.fit(X_train,y_train)\n",
        "y_pred=knn.predict(X_test)\n",
        "\n",
        "print(accuracy_score(y_pred,y_test))"
      ],
      "metadata": {
        "id": "Yv1a1dAkM5ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OBSERVATION for KNN**\n",
        "\n",
        "*The accuracy is 1*\n",
        "\n",
        "*the accuracy is around 0.9954251459220698*"
      ],
      "metadata": {
        "id": "_xxHjTbW1FF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ELBOW METHOD**\n",
        "\n",
        "**Now we will plot the graph for accuracy and k_neighbours**"
      ],
      "metadata": {
        "id": "ovet9tA82Sur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores=[]\n",
        "\n",
        "for i in range(1, 16):\n",
        "  knn=KNeighborsClassifier(n_neighbors=i)\n",
        "  knn.fit(X_train,y_train)\n",
        "  y_pred=knn.predict(X_test)\n",
        "  scores.append(accuracy_score(y_pred,y_test))\n",
        "\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "awi6aebl1CBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*It is perfectly working fine with all the n_neighbours so it is ok to go with the default configuration(which is 5 as per the documentation)*"
      ],
      "metadata": {
        "id": "Zgg5UsDn3SyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*We can also plot a graph however in this case we will get a stright line parallel to X-axis*"
      ],
      "metadata": {
        "id": "617FeYKo3oRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.title('Graph of n_neighbours vs accuracy')\n",
        "plt.plot(range(1,16),scores)\n",
        "plt.xlabel('n_neighbours')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "FLMpsNX03Bbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*we are getting maximum accuracy when n_neighbours is 2*"
      ],
      "metadata": {
        "id": "e57Fy7jEzQEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**applying KNN with n_neighbours=2**\n",
        "*n_neighbour is a hyperparameter in KNN*"
      ],
      "metadata": {
        "id": "BHDm0es-z1AZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scalar=StandardScaler()\n",
        "\n",
        "X_train=scalar.fit_transform(X_train)\n",
        "X_test=scalar.transform(X_test)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "#no is neighbours is 5 - default config.\n",
        "knn=KNeighborsClassifier(n_neighbors=2)\n",
        "\n",
        "knn.fit(X_train,y_train)\n",
        "y_pred=knn.predict(X_test)\n",
        "\n",
        "print(accuracy_score(y_pred,y_test))"
      ],
      "metadata": {
        "id": "y9VYRL2_0BTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*accuracy improved from 9954251459220698 to\n",
        "0.996844928222117*"
      ],
      "metadata": {
        "id": "VJBtlqYx0IkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machine(SVM)"
      ],
      "metadata": {
        "id": "PiutktLy414p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "\n",
        "# assign test data size 25%\n",
        "X_train, X_test, y_train, y_test =train_test_split(X_selected,y_selected,test_size= 0.3, random_state=0)\n",
        "\n",
        "# importing standard scaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# scalling the input data\n",
        "sc_X = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.fit_transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "svm_clf =svm.SVC()\n",
        "\n",
        "# training the model\n",
        "svm_clf.fit(X_train, y_train)\n",
        "# testing the model\n",
        "y_pred1 = svm_clf.predict(X_test)\n",
        "\n",
        "# importing accuracy score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(classification_report(y_test, y_pred1))\n",
        "print(confusion_matrix(y_test, y_pred1))\n",
        "print(accuracy_score(y_test, y_pred1))"
      ],
      "metadata": {
        "id": "DwqlmY5g1HuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we are able to achieve 97 percent accuracy using Linear SVM without hyper-parameter tunning"
      ],
      "metadata": {
        "id": "HRLFMEZc1dhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayees"
      ],
      "metadata": {
        "id": "H2woJtRt5Av-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-ldkLl3FYmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# assign test data size 25%\n",
        "X_train, X_test, y_train, y_test =train_test_split(X_selected,y_selected,test_size= 0.3, random_state=0)\n",
        "\n",
        "# importing standard scaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# scalling the input data\n",
        "sc_X = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.fit_transform(X_test)\n",
        "\n",
        "# import Gaussian Naive Bayes classifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "# create a Gaussian Classifier\n",
        "classifer1 = GaussianNB()\n",
        "# training the model\n",
        "classifer1.fit(X_train, y_train)\n",
        "# testing the model\n",
        "y_pred1 = classifer1.predict(X_test)\n",
        "\n",
        "# importing accuracy score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(classification_report(y_test, y_pred1))\n",
        "print(confusion_matrix(y_test, y_pred1))\n",
        "print(accuracy_score(y_test, y_pred1))"
      ],
      "metadata": {
        "id": "e9R4plE232na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The accuracy achived using Naive Bayees is 96 percent*"
      ],
      "metadata": {
        "id": "bj3VGlRg0c1u"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9jWxf5yGHTg0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}